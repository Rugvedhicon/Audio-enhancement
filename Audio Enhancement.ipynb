{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Rugvedh\\New folder\\Lib\\site-packages\\librosa\\core\\intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_filename\n",
            "c:\\Users\\Rugvedh\\New folder\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading AudioSR: basic\n",
            "Loading model on cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Rugvedh\\New folder\\Lib\\site-packages\\torch\\functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4324.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "c:\\Users\\Rugvedh\\New folder\\Lib\\site-packages\\torchaudio\\transforms\\_transforms.py:581: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DiffusionWrapper has 258.20 M params.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Rugvedh\\New folder\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 0 training pairs\n",
            "Epoch 1 | Loss: 0.0000\n",
            "Epoch 2 | Loss: 0.0000\n",
            "Epoch 3 | Loss: 0.0000\n",
            "✅ Saved fine_tuned_audiosr.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import librosa\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from audiosr import build_model\n",
        "\n",
        "# Folders\n",
        "LOW_FOLDER = \"low\"\n",
        "HIGH_FOLDER = \"high\"\n",
        "\n",
        "# Device (CPU/GPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load pretrained model\n",
        "model = build_model(model_name=\"basic\", device=device).to(device)\n",
        "\n",
        "# Loss + optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Load training pairs\n",
        "pairs = []\n",
        "for file in os.listdir(LOW_FOLDER):\n",
        "    if file.endswith(\".wav\"):\n",
        "        low_path = os.path.join(LOW_FOLDER, file)\n",
        "        high_path = os.path.join(HIGH_FOLDER, file)\n",
        "        if os.path.exists(high_path):\n",
        "            # Load audio\n",
        "            y_low, sr_low = librosa.load(low_path, sr=None)\n",
        "            y_high, sr_high = librosa.load(high_path, sr=None)\n",
        "\n",
        "            # Resample to same rate\n",
        "            target_sr = 12000\n",
        "            y_low = librosa.resample(y_low, sr_low, target_sr)\n",
        "            y_high = librosa.resample(y_high, sr_high, target_sr)\n",
        "\n",
        "            # Convert to tensors\n",
        "            low_tensor = torch.tensor(y_low, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            high_tensor = torch.tensor(y_high, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "            pairs.append((low_tensor, high_tensor))\n",
        "\n",
        "print(f\"Loaded {len(pairs)} training pairs\")\n",
        "\n",
        "# Fine-tune\n",
        "for epoch in range(3):  # small number of epochs\n",
        "    total_loss = 0.0\n",
        "    for low, high in pairs:\n",
        "        output = model(low)\n",
        "        loss = criterion(output, high)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), \"fine_tuned_audiosr.pth\")\n",
        "print(\"✅ Saved fine_tuned_audiosr.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading AudioSR: basic\n",
            "Loading model on cpu\n",
            "DiffusionWrapper has 258.20 M params.\n",
            "Running DDIM Sampling with 50 timesteps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DDIM Sampler: 100%|██████████| 50/50 [15:53<00:00, 19.06s/it] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Enhanced audio saved as enhanced_output.wav (48 kHz)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import soundfile as sf\n",
        "from audiosr import build_model, super_resolution\n",
        "\n",
        "# Load model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = build_model(model_name=\"basic\", device=device)\n",
        "\n",
        "# Load fine-tuned weights\n",
        "model.load_state_dict(torch.load(\"fine_tuned_audiosr.pth\", map_location=device))\n",
        "\n",
        "# Run super-resolution\n",
        "audios = super_resolution(\n",
        "    model,\n",
        "    \"song.wav\",   # your low-quality 12kHz file\n",
        "    seed=42,\n",
        "    guidance_scale=3.5,\n",
        "    ddim_steps=50,\n",
        "    latent_t_per_second=12.8\n",
        ")\n",
        "\n",
        "# Convert tensor -> numpy\n",
        "enhanced = audios[0]  # first sample\n",
        "if isinstance(enhanced, torch.Tensor):\n",
        "    enhanced = enhanced.cpu().numpy()\n",
        "\n",
        "# Fix shape (make it [samples, channels] if needed)\n",
        "if enhanced.ndim == 2 and enhanced.shape[0] < enhanced.shape[1]:\n",
        "    enhanced = enhanced.T  # transpose to (samples, channels)\n",
        "\n",
        "# Save at 48kHz\n",
        "sf.write(\"enhanced_output.wav\", enhanced, 48000)\n",
        "print(\"✅ Enhanced audio saved as enhanced_output.wav (48 kHz)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "# Load the audio file\n",
        "data, samplerate = sf.read('audio.wav')\n",
        "\n",
        "print(\"Sample Rate:\", samplerate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (mlprac_env)",
      "language": "python",
      "name": "mlprac_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
